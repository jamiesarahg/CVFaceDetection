Writeup for CV Project
Jamie Gorson and Nicole Rifkin

Video can be found here: https://www.youtube.com/watch?v=S-CmvTUCd8c

What was the goal of your project?
The goal of our project was to control a neato with facial recognition. Our program would track the yaw and pitch of our face in order to determine the speed and direction of the neato. 

Describe how your system works.  Make sure to include the basic components and algorithms that comprise your project.
Our system is comprised of a laptop camera, which uses a face classifier to identify faces in the image. The face is then run through a model, which was determined from a data set using ridge regression. This model determines the yaw and pitch of the incoming image. This is then sent to the neato to control its motion.

Describe a design decision you had to make when working on your project and what you ultimately did (and why)? These design decisions could be particular choices for how you implemented some part of an algorithm or perhaps a decision regarding which of two external packages to use in your project.
We spent a lot of time thinking about how to collect the best data for training a model on determining yaw and pitch. We knew that we needed a dataset that could be used for supervised learning and that included a wide variety of faces and head orientations. Our solution to data collection was to use pygame to move a small dot around a black screen for about 30 seconds. We asked our volunteer face models to sit in front of the camera and follow the dot with their head. We tried to keep lighting consistent (with the model always facing the brightest light source) and assumed the the position of the dot closely represented the angle of the head. Deciding what data to capture was also fairly important. We didn't want too many images because they would be repetitive, unnecessary and take up extra space, but we didn't want too little or we wouldn't have enough data to train on. We captured 32 images of each model at regular intervals and automatically labeled them based on dot position. We then cropped the images to have square dimensions and include only the face, and averaged groups of pixels to create a compressed 24x24 image. We saved them as a 3 channel color image, but when fitting them to the model, we converted them to a 1 channel black and white image for a less complex training set. The images were saved named by their timestamp in a complex folder structure that denotes the pixel value of the dot at the moment the image was taken.



How did you structure your code?

There were two main components to our program at a high level: handling the training data and the prediction data. 
The class Predictor (from the script prediction) is the part of our code that should be run using rosrun. It runs prediction data through the ridge regression model, calculates a trailing average of prior predictions to reduce noise, converts the prediction to a robot speed, and publishes an update to the robot. 
The class DataCollection (from the script DataCollection) handles the pygame dot screen update and data intake. It is responsible for labelling and saving training data. 
Both classes heavily utilize  the class ImageManipulation (found in the script ImageManipulation) for capturing, displaying, saving, cropping, compressing, or otherwise manipulating images. 
The ImageManipulation class contains a Camera class, which is a small class that handles camera intialization and contains the face classifier. Camera is also found in the ImageManipulation script. 
The class RidgeModel (found in the script RidgeRegression) is called by the Predictor class. It takes in all training data and fits a model. That model becomes an attribute of the predictor. 
Both DataCollection and RidgeModel inherit from the class DirManager, a directory manager that handles our folder structure that labels the data. We included this inheritence because DirManager has a method to return all images and their labels in the training data directory, which is used for compressing data in DataCollection and for reading data in RidgeModel. 
We originally started out writing this functionally in order to wrap our heads around the task. We decided that there were too many parameters that had to be passed back and forth and that it would be more readable and efficient in object oriented code. When we realized how much code could be reused between writing data intake and writing the predictor, it became easier to do a restructure. There are still places, especially in ImageManipulation or Camera, where functions could be rethought to be more modular, reduce the total amount of code and take advantage of the object oriented capabilities. We also didn't convert everything to object oriented. One of those things would be the facial recognition code, which is explained in the next question. 


What if any challenges did you face along the way?
Face recognition was a challenge that we faced, since we took on a large challenge like that at a very late stage of the project. We were able to get it to work sometimes, but we would like to spend more time parameter turning and exploring with the different approaches. Due to time constraints, we had to pick one path to go down and do our best to implement it. Since neither of us have experience with facial recognition, it was a challenge, but I'm happy with the bit we were able to accomplish. From what we can tell there are a few problems with the methods that we used for facial recognition. Each person's face that we have data for is taken in the same light, have the same expression, and has the same make-up or hair-do. Finally, the camera is coming from the same angle! So, if any of these things change when the system tries to recognize the face later, then, it could easily be understood as the wrong person. In the future, if we were to be doing facial recognition, we would need a more distributed data set with these variables accounted for. So, as much as we want to attempt parameter tuning, the only way to make our facial recognition better would be to collect a larger, more varied data-set. Because of this, when we converted our code to object oriented, we did not include the facial recognition (mostly in deciding where time would be best spent). But, by running test_face_recognition.py, you can see the cross validated results of the facial recognition, and in the functional code of realTimePrediction.py, you can see the code for it to work in conjunction with the system.

What would you do to improve your project if you had more time?
If we had more time, we would like to do two specific things. The first is make our program react faster. Because of intaking the images and the computation that we have to do with them, this runs relatively slowly, and the robot reacts slowly to head movement. While we have taken steps to address this, we wish that it could be even faster and don't believe we solved it completely. Of course, we would really like to improve the facial recognition aspect. Since it was a cool add-on to our project and not crucial to its success, we were not able to tune it as well as we would have liked, however if more research was done into facial recognition, we might be able to get a better model working.

Did you learn any interesting lessons for future robotic programming projects? These could relate to working on robotics projects in teams, working on more open-ended (and longer term) problems, or any other relevant topic.
Creating a dataset was one of the larger tasks of this project. We put in a lot of thought to how we designed the intake of data to both have well stored and documented information as well as good facial images. This experience was very informative for future projects for making datasets as well as interpreting them.
